{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f29f528-c28d-46cd-9b40-364fc1f34280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (1.26.2)\n",
      "Requirement already satisfied: matplotlib in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in /home/u.sk336075/.local/lib/python3.11/site-packages (0.13.2)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.4.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/u.sk336075/.local/lib/python3.11/site-packages (1.7.2)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /home/u.sk336075/.local/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m866.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/u.sk336075/.local/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/u.sk336075/.local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /sw/hprc/sw/Anaconda/2023.09-0/envs/default_jupyterlab/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading plotly-6.4.0-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.9/547.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.11.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, narwhals, click, plotly, nltk, wordcloud\n",
      "Successfully installed click-8.3.0 narwhals-2.11.0 nltk-3.9.2 plotly-6.4.0 regex-2025.11.3 wordcloud-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/u.sk336075/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/u.sk336075/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn plotly nltk scikit-learn wordcloud\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d837c124-1026-47e9-b111-033ad579348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for new files to be uploaded...\n",
      "Expected files: train.csv, test.csv, sample_submission.csv\n",
      "Current files in data folder: ['test.csv', 'train.csv', 'sample_submission.csv', '.ipynb_checkpoints']\n",
      "Files after waiting: ['test.csv', 'train.csv', 'sample_submission.csv', '.ipynb_checkpoints']\n",
      "\n",
      "Loading complete training dataset...\n",
      "Complete training dataset loaded successfully!\n",
      "Dataset shape: (7613, 5)\n",
      "Columns: ['id', 'keyword', 'location', 'text', 'target']\n",
      "\n",
      "First 2 rows with text:\n",
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n"
     ]
    }
   ],
   "source": [
    "# Checking for the new files\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Waiting for new files to be uploaded...\")\n",
    "print(\"Expected files: train.csv, test.csv, sample_submission.csv\")\n",
    "\n",
    "# Check for files in data folder\n",
    "data_files = os.listdir('../data')\n",
    "print(\"Current files in data folder:\", data_files)\n",
    "\n",
    "# Wait a moment for upload to complete\n",
    "time.sleep(2)\n",
    "\n",
    "# Check again\n",
    "data_files = os.listdir('../data')\n",
    "print(\"Files after waiting:\", data_files)\n",
    "\n",
    "if 'train.csv' in data_files:\n",
    "    print(\"\\nLoading complete training dataset...\")\n",
    "    train_df = pd.read_csv('../data/train.csv')\n",
    "    print(\"Complete training dataset loaded successfully!\")\n",
    "    print(\"Dataset shape:\", train_df.shape)\n",
    "    print(\"Columns:\", train_df.columns.tolist())\n",
    "    \n",
    "    # Quick preview\n",
    "    print(\"\\nFirst 2 rows with text:\")\n",
    "    print(train_df[['text', 'target']].head(2))\n",
    "else:\n",
    "    print(\"\\n train.csv not found yet\")\n",
    "    print(\"Please make sure train.csv is uploaded to ../data/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbc96ed-452f-42c4-8941-38074d7f877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE DATASET ANALYSIS\n",
      "========================================\n",
      "Dataset dimensions: (7613, 5)\n",
      "Number of samples: 7613\n",
      "Number of features: 5\n",
      "\n",
      "Column details:\n",
      "id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "target       int64\n",
      "dtype: object\n",
      "\n",
      "Missing values analysis:\n",
      "          Missing Count  Missing Percentage\n",
      "id                    0            0.000000\n",
      "keyword              61            0.801261\n",
      "location           2533           33.272035\n",
      "text                  0            0.000000\n",
      "target                0            0.000000\n",
      "\n",
      "Target distribution:\n",
      "        Count  Percentage\n",
      "target                   \n",
      "0        4342   57.034021\n",
      "1        3271   42.965979\n",
      "\n",
      "Sample disaster tweets (target=1):\n",
      "1. Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "2. Forest fire near La Ronge Sask. Canada\n",
      "3. All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "\n",
      "Sample non-disaster tweets (target=0):\n",
      "1. What's up man?\n",
      "2. I love fruits\n",
      "3. Summer is lovely\n"
     ]
    }
   ],
   "source": [
    "# Basic dataset inspection\n",
    "print(\"COMPLETE DATASET ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"Dataset dimensions:\", train_df.shape)\n",
    "print(\"Number of samples:\", len(train_df))\n",
    "print(\"Number of features:\", train_df.shape[1])\n",
    "\n",
    "print(\"\\nColumn details:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values analysis:\")\n",
    "missing_data = train_df.isnull().sum()\n",
    "missing_percent = (train_df.isnull().sum() / len(train_df)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "print(missing_summary)\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "target_counts = train_df['target'].value_counts()\n",
    "target_percent = train_df['target'].value_counts(normalize=True) * 100\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent\n",
    "})\n",
    "print(target_summary)\n",
    "\n",
    "print(\"\\nSample disaster tweets (target=1):\")\n",
    "disaster_tweets = train_df[train_df['target'] == 1]['text'].head(3)\n",
    "for i, tweet in enumerate(disaster_tweets, 1):\n",
    "    print(f\"{i}. {tweet}\")\n",
    "\n",
    "print(\"\\nSample non-disaster tweets (target=0):\")\n",
    "non_disaster_tweets = train_df[train_df['target'] == 0]['text'].head(3)\n",
    "for i, tweet in enumerate(non_disaster_tweets, 1):\n",
    "    print(f\"{i}. {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f13d68e-f430-48a9-b67d-f88b190ff87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data\n",
      "punkt_tab downloaded successfully\n",
      "Simplified preprocessing function created successfully\n",
      "\n",
      "Testing simplified preprocessing:\n",
      "Original tweet: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all http://example.com\n",
      "After cleaning: our deeds reason this earthquake may allah forgive us all\n",
      "\n",
      "Testing on actual dataset samples:\n",
      "Sample 1:\n",
      "  Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "  Cleaned: our deeds reason this earthquake may allah forgive us all\n",
      "\n",
      "Sample 2:\n",
      "  Original: Forest fire near La Ronge Sask. Canada\n",
      "  Cleaned: forest fire near la ronge sask canada\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/u.sk336075/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Fixing NLTK download issues and creating robust preprocessing\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading required NLTK data\")\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "    print(\"punkt_tab downloaded successfully\")\n",
    "except:\n",
    "    print(\"Using alternative approach\")\n",
    "\n",
    "# Creating a simpler preprocessing function that doesn't rely on advanced tokenization\n",
    "def clean_tweet_text_simple(text):\n",
    "    \"\"\"\n",
    "    Clean tweet text using simple string operations\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove hashtags but keep the text\n",
    "        text = re.sub(r'#', '', text)\n",
    "        \n",
    "        # Remove special characters and digits, keep only letters and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Simple stopwords removal using string operations\n",
    "        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', 'was', 'were', \n",
    "                     'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'as', 'it'}\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        text = ' '.join(filtered_words)\n",
    "        \n",
    "    return text\n",
    "\n",
    "print(\"Simplified preprocessing function created successfully\")\n",
    "\n",
    "# Testing the simplified preprocessing\n",
    "test_tweet = \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all http://example.com\"\n",
    "print(\"\\nTesting simplified preprocessing:\")\n",
    "print(\"Original tweet:\", test_tweet)\n",
    "print(\"After cleaning:\", clean_tweet_text_simple(test_tweet))\n",
    "\n",
    "# Testing on actual dataset samples\n",
    "print(\"\\nTesting on actual dataset samples:\")\n",
    "sample_tweets = train_df['text'].head(2)\n",
    "for i, tweet in enumerate(sample_tweets):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"  Original:\", tweet)\n",
    "    print(\"  Cleaned:\", clean_tweet_text_simple(tweet))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d80b2a9-a9e7-4419-bb0d-60cae85f6f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing of entire dataset\n",
      "Applying text cleaning to all tweets...\n",
      "Handling missing values...\n",
      "Creating text length features...\n",
      "Preprocessing completed successfully\n",
      "New dataset shape: (7613, 8)\n",
      "New columns: ['id', 'keyword', 'location', 'text', 'target', 'cleaned_text', 'text_length', 'char_length']\n",
      "\n",
      "Preprocessing results summary:\n",
      "Original text sample:\n",
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Cleaned text sample:\n",
      "our deeds reason this earthquake may allah forgive us all\n",
      "Text length: 10\n",
      "Character length: 57\n",
      "\n",
      "Missing values after preprocessing:\n",
      "id              0\n",
      "keyword         0\n",
      "location        0\n",
      "text            0\n",
      "target          0\n",
      "cleaned_text    0\n",
      "text_length     0\n",
      "char_length     0\n",
      "dtype: int64\n",
      "\n",
      "Dataset info after preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            7613 non-null   int64 \n",
      " 1   keyword       7613 non-null   object\n",
      " 2   location      7613 non-null   object\n",
      " 3   text          7613 non-null   object\n",
      " 4   target        7613 non-null   int64 \n",
      " 5   cleaned_text  7613 non-null   object\n",
      " 6   text_length   7613 non-null   int64 \n",
      " 7   char_length   7613 non-null   int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 475.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Applying preprocessing to the entire dataset\n",
    "print(\"Starting preprocessing of entire dataset\")\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "processed_df = train_df.copy()\n",
    "\n",
    "print(\"Applying text cleaning to all tweets...\")\n",
    "processed_df['cleaned_text'] = processed_df['text'].apply(clean_tweet_text_simple)\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "processed_df['keyword'] = processed_df['keyword'].fillna('unknown')\n",
    "processed_df['location'] = processed_df['location'].fillna('unknown')\n",
    "\n",
    "print(\"Creating text length features...\")\n",
    "processed_df['text_length'] = processed_df['cleaned_text'].apply(lambda x: len(str(x).split()))\n",
    "processed_df['char_length'] = processed_df['cleaned_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "print(\"Preprocessing completed successfully\")\n",
    "print(\"New dataset shape:\", processed_df.shape)\n",
    "print(\"New columns:\", processed_df.columns.tolist())\n",
    "\n",
    "# Display preprocessing results\n",
    "print(\"\\nPreprocessing results summary:\")\n",
    "print(\"Original text sample:\")\n",
    "print(train_df['text'].iloc[0])\n",
    "print(\"Cleaned text sample:\")\n",
    "print(processed_df['cleaned_text'].iloc[0])\n",
    "print(\"Text length:\", processed_df['text_length'].iloc[0])\n",
    "print(\"Character length:\", processed_df['char_length'].iloc[0])\n",
    "\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(processed_df.isnull().sum())\n",
    "\n",
    "print(\"\\nDataset info after preprocessing:\")\n",
    "print(processed_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef3edb3-0714-48d1-a2ea-cbcff94b2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and validation sets\n",
      "Data splitting completed successfully\n",
      "Training set size: 6090 samples (80.0%)\n",
      "Validation set size: 1523 samples (20.0%)\n",
      "\n",
      "Training set target distribution:\n",
      "        Count  Percentage\n",
      "target                   \n",
      "0        3473   57.027915\n",
      "1        2617   42.972085\n",
      "\n",
      "Validation set target distribution:\n",
      "        Count  Percentage\n",
      "target                   \n",
      "0         869   57.058437\n",
      "1         654   42.941563\n",
      "\n",
      "Saving processed datasets...\n",
      "Processed data saved successfully\n",
      "Files created:\n",
      "- ../data/processed_train.csv (full processed dataset)\n",
      "- ../data/train_split.csv (training split)\n",
      "- ../data/val_split.csv (validation split)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Splitting data into training and validation sets\")\n",
    "\n",
    "# Preparing features and target\n",
    "X = processed_df['cleaned_text']  # Using cleaned text as features\n",
    "y = processed_df['target']        # Target variable\n",
    "\n",
    "# Performing the split (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintaining same target distribution in both sets\n",
    ")\n",
    "\n",
    "print(\"Data splitting completed successfully\")\n",
    "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining set target distribution:\")\n",
    "train_target_counts = y_train.value_counts()\n",
    "train_target_percent = y_train.value_counts(normalize=True) * 100\n",
    "print(pd.DataFrame({\n",
    "    'Count': train_target_counts,\n",
    "    'Percentage': train_target_percent\n",
    "}))\n",
    "\n",
    "print(\"\\nValidation set target distribution:\")\n",
    "val_target_counts = y_val.value_counts()\n",
    "val_target_percent = y_val.value_counts(normalize=True) * 100\n",
    "print(pd.DataFrame({\n",
    "    'Count': val_target_counts,\n",
    "    'Percentage': val_target_percent\n",
    "}))\n",
    "\n",
    "# Saving the processed data\n",
    "print(\"\\nSaving processed datasets...\")\n",
    "processed_df.to_csv('../data/processed_train.csv', index=False)\n",
    "\n",
    "# Creating separate files for train and validation splits\n",
    "train_split = processed_df.loc[X_train.index]\n",
    "val_split = processed_df.loc[X_val.index]\n",
    "\n",
    "train_split.to_csv('../data/train_split.csv', index=False)\n",
    "val_split.to_csv('../data/val_split.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved successfully\")\n",
    "print(\"Files created:\")\n",
    "print(\"- ../data/processed_train.csv (full processed dataset)\")\n",
    "print(\"- ../data/train_split.csv (training split)\")\n",
    "print(\"- ../data/val_split.csv (validation split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7f407f-3318-4cd3-8027-d2ca16711054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK 1: DATA PREPROCESSING - COMPLETED\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "✓ Original dataset: 7613 samples, 5 columns\n",
      "✓ Processed dataset: 7613 samples, 8 columns\n",
      "✓ Missing values handled: 0 remaining\n",
      "✓ Text cleaning applied to all 7613 tweets\n",
      "✓ New features created: text_length, char_length\n",
      "✓ Data split: 6090 training, 1523 validation samples\n",
      "✓ Class balance maintained in both splits\n",
      "\n",
      "FILES GENERATED:\n",
      "✓ ../data/processed_train.csv - Full processed dataset\n",
      "✓ ../data/train_split.csv - Training split (80%)\n",
      "✓ ../data/val_split.csv - Validation split (20%)\n",
      "\n",
      "NEXT STEPS:\n",
      "→ Proceed to Notebook 2: EDA and Visualization\n",
      "→ Analyze text length distributions\n",
      "→ Create word frequency visualizations\n",
      "→ Generate class distribution charts\n",
      "\n",
      "DATA PREPROCESSING PHASE COMPLETED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "# Final summary for Notebook 1: Data Preprocessing\n",
    "print(\"NOTEBOOK 1: DATA PREPROCESSING - COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"PROCESSING SUMMARY:\")\n",
    "print(f\"✓ Original dataset: {len(train_df)} samples, {train_df.shape[1]} columns\")\n",
    "print(f\"✓ Processed dataset: {len(processed_df)} samples, {processed_df.shape[1]} columns\")\n",
    "print(f\"✓ Missing values handled: {processed_df.isnull().sum().sum()} remaining\")\n",
    "print(f\"✓ Text cleaning applied to all {len(processed_df)} tweets\")\n",
    "print(f\"✓ New features created: text_length, char_length\")\n",
    "print(f\"✓ Data split: {len(X_train)} training, {len(X_val)} validation samples\")\n",
    "print(f\"✓ Class balance maintained in both splits\")\n",
    "\n",
    "print(\"\\nFILES GENERATED:\")\n",
    "print(\"✓ ../data/processed_train.csv - Full processed dataset\")\n",
    "print(\"✓ ../data/train_split.csv - Training split (80%)\")\n",
    "print(\"✓ ../data/val_split.csv - Validation split (20%)\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"→ Proceed to Notebook 2: EDA and Visualization\")\n",
    "print(\"→ Analyze text length distributions\")\n",
    "print(\"→ Create word frequency visualizations\")\n",
    "print(\"→ Generate class distribution charts\")\n",
    "\n",
    "print(\"\\nDATA PREPROCESSING PHASE COMPLETED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fd002-812c-4cf6-a19f-e996e637ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
